---
layout: post
title:  "论文阅读-CycleGAN:Unpaired_Image-to-Image_Translation"
date:   2019-03-06 19:48:07 +0800--
categories: []
tags: []  
---
吸取之前没放原文和代码的教训，链接先行：
- Paper：[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)  
- Codes：[https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)  

CycleGAN，不是什么新鲜事了，写这篇也只是做个周全，最近看论文看的要吐，写个读后感缓一缓～～

先来张图一睹为快：
![CycleGAN_glimpse](https://s1.ax2x.com/2019/03/06/5GDXeh.png)  

这篇论文应该说就是冲着[pix2pix](https://www.oukohou.wang/2019/01/07/Image-to-Image-Translation-with-Conditional-Adversarial-Networks/)
来的，针对pix2pix硬性要求训练集是成对的图像（paired images）的问题，实现了一个不需要成对图像的框架。  

### 1. issue:   
就是pix2pix的老大难问题：
- 数据集必须是成对的。  

但我们都知道，深度学习的一大难题就是数据集不好处理，结果你pix2pix倒好，不仅要有数据，还要必须是成对的数据？  
这也正是CycleGAN要着手解决的痛点：  
![paired vs unpaired data](https://s1.ax2x.com/2019/03/06/5GDmgH.png)  

### 2. solution:  
解决方案也很直白，但有一说一，人家想出这样的解决方案，还是很厉害的：  
- 汉语翻译成日语，再翻译汉语，那么得到的汉语应该跟最开始的汉语是一样的；  
- 那么同理，一张图像，翻译成另一张图像，再翻译回来得到的图像，也应该和原图保持一致性。  

即：$x \to G(x) \to F(G(x)) \approx x$.  
好了，完了。  

### 3. architecture：  
当然，这样就完了的话，就有点耍流氓了，还是要多说两句。    
不过理解了[前面](#2-solution)的意思，那么其实这篇论文也就差不多了。  
论文框架结构如下：  
![architecture](https://s1.ax2x.com/2019/03/06/5GDV0N.png)  

其在两个domains $X$和$Y$之间做了两个transfer：  
- $X \to Y$;  
- $Y \to X$.  

其损失函数，首先是免不了的adversarial loss:  
　　$$L_{GAN}(G,D_Y,X,Y)=\mathbb{E}_{y\sim P_{data}(y)}[logD_Y(y))]+\mathbb{E}_{x\sim P_{data}(x))}[log(1-D_Y(G(x))]$$  
不过，注意到这只是$X \to Y$的，反方向也要有一个：  
　　$$L_{GAN}(F,D_X,Y,X)=\mathbb{E}_{x\sim P_{data}(x)}[logD_X(x))]+\mathbb{E}_{y\sim P_{data}(y)}[log(1-D_X(F(y))]$$  
然后是根据 `Cycle` 的思想引出来的 `Cycle Consistency Loss`：  
　　$$L_{cyc}(G,F)=\mathbb{E}_{x\sim P_{data}(x)}[||F(G(x))-x||_1]+\mathbb{E}_{y\sim P_{data}(y)}[||G(F(y))-y||_1]$$  
所以最终的损失函数长这样：  
![full_loss](https://s1.ax2x.com/2019/03/06/5GDYau.png)  


### 4. implementation：  
实现其实也没啥好说，不过值得一提的是CycleGAN论文中提到了这篇论文：[Learning from Simulated and Unsupervised Images through Adversarial
Training](http://openaccess.thecvf.com/content_cvpr_2017/papers/Shrivastava_Learning_From_Simulated_CVPR_2017_paper.pdf)
中用到的一个trick：  
![Illustration of using a history of refined images](https://s1.ax2x.com/2019/03/06/5GD0f9.png)    

什么意思呢？就是在训练Discriminator的时候，不再是仅仅把Generator最新生成的图片作为训练集喂给D，而是加上缓存的一些G在之前生成的图片，
这样可以一定程度上避免模型震荡。  
恩，还不错的trick。  

### 5. results:
接下来就是大家喜闻乐见的有图有真相环节，这个没啥好说，无非就是什么数据集，什么衡量指标，我们是STOA blahblah……  
直接放图吧：  
![style_transfer](https://s1.ax2x.com/2019/03/06/5GDWTA.png)  
![season_transfer](https://s1.ax2x.com/2019/03/06/5GDj6q.png)  
![horse_vs_zebra](https://s1.ax2x.com/2019/03/06/5GDdJO.png)  

好啦，放了三張啦，想要看更多的话，可以访问论文作者的网页：[CycleGAN](https://junyanz.github.io/CycleGAN/).      

以上，完结撒花～～～  





<br>
微信公众号：璇珠杂俎(也可搜索[oukohou](https://mp.weixin.qq.com/s/dCxGcuv5ngyR6U-uBYVI9Q))，提供本站优质非技术博文～～
[![wechat_official_account](https://www.oukohou.wang/assets/imgs/wechat_official_account.png)](https://mp.weixin.qq.com/s/dCxGcuv5ngyR6U-uBYVI9Q "点击图像直达微信公众号～～")  




<br>
<p  align="right">regards.</p>
<h4 align="right">
    <a href="https://www.oukohou.wang/">
        oukohou.
    </a>
</h4>

